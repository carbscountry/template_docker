version: '3.2'
services:
  app:
    build:
      context: .
      shm_size: '5gb'


    # dockerfile: Dockerfile
    container_name: 'cuda_python'
    devices:
      - /dev/nvidia0
    working_dir: '/workspace'
    volumes:
      -  ./:/workspace/
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ports:
      - "127.0.0.1:8080:8888"
    tty: true
